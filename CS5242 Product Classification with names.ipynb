{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53e22fb",
   "metadata": {},
   "source": [
    "# Shopee Product Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00b4f80",
   "metadata": {},
   "source": [
    "Notebook to experiment over several Neural Networks over the product dataset obtained from Shopee and evaluate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa5624",
   "metadata": {},
   "source": [
    "The following models are evaluated as part of this notebook:\n",
    "\n",
    "Baseline 1: CNN <br>\n",
    "Baseline 2: CNN with augmented layers <br>\n",
    "Improvement 1: Adding RNN <br>\n",
    "Improvement 2: Adding attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e52a97",
   "metadata": {},
   "source": [
    "## Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97c7e4bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (0.19.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from scikit-image) (1.7.1)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from scikit-image) (2.6.3)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from scikit-image) (2021.11.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from scikit-image) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from scikit-image) (1.21.3)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from scikit-image) (2.22.2)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from scikit-image) (8.4.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from scikit-image) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from packaging>=20.0->scikit-image) (2.4.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shopee_crawler in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (0.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\python.exe -m pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 21.3; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from torchvision) (3.10.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: torch==1.12.1 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from torchvision) (1.12.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from torchvision) (1.21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from requests->torchvision) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (4.6.0.66)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\shubh\\.pyenv\\pyenv-win\\versions\\3.7.9\\lib\\site-packages (from opencv-python) (1.21.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image\n",
    "!pip install shopee_crawler\n",
    "!pip install torchvision\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7320f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import importlib\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b26a1",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f858a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'data/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b887334e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a686c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.text import dataset\n",
    "from model.dataset import all_categories\n",
    "from scripts.crawler import product_category_and_names\n",
    "\n",
    "_, product_names = product_category_and_names('data')\n",
    "\n",
    "data = dataset.DataSet(product_names, max_num_img=300, categories=all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c83bd4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model.text.dataset' from '/Users/naomileow/Documents/school/CS5242/project/model/text/dataset.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712aabfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 437.08it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 441.97it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 510.00it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 523.07it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 525.23it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 502.45it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 505.85it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 535.33it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 553.94it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 477.70it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 479.13it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 510.68it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 480.28it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 451.32it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 471.08it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 459.94it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 496.87it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 523.05it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 508.81it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 478.67it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 473.13it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 526.48it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 440.67it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 477.41it/s]\n"
     ]
    }
   ],
   "source": [
    "data.load_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d192805",
   "metadata": {},
   "source": [
    "## CNN Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e3ef77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.text import trainer, baseline\n",
    "\n",
    "batch_size = 32\n",
    "datastore = trainer.DataStore(data, batch_size)\n",
    "\n",
    "baseline_model = baseline.BaselineCNN(len(data.categories), datastore.vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=5e-4)\n",
    "mtrainer = trainer.Trainer(baseline_model, optimizer, criterion, batch_size, datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aff205c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model.text.baseline' from '/Users/naomileow/Documents/school/CS5242/project/model/text/baseline.py'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(trainer)\n",
    "importlib.reload(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89ba372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs5242-proj/lib/python3.9/site-packages/torch/nn/functional.py:2380: UserWarning: The operator 'aten::_embedding_bag' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  ret, _, _, _ = torch.embedding_bag(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0 |   100 batches loss: 2.9758\n",
      "[Epoch   0]: Training loss: 2.943993 | Accuracy: 0.130159\n",
      "[Epoch   0]: Validation loss: 2.919566 | Accuracy: 0.137500 | Within 3: 0.323611\n",
      "epoch   1 |   100 batches loss: 2.7790\n",
      "[Epoch   1]: Training loss: 2.766027 | Accuracy: 0.194444\n",
      "[Epoch   1]: Validation loss: 2.700684 | Accuracy: 0.226389 | Within 3: 0.431944\n",
      "epoch   2 |   100 batches loss: 2.5695\n",
      "[Epoch   2]: Training loss: 2.542722 | Accuracy: 0.275794\n",
      "[Epoch   2]: Validation loss: 2.446675 | Accuracy: 0.290278 | Within 3: 0.550000\n",
      "epoch   3 |   100 batches loss: 2.2887\n",
      "[Epoch   3]: Training loss: 2.239562 | Accuracy: 0.375595\n",
      "[Epoch   3]: Validation loss: 2.183918 | Accuracy: 0.380556 | Within 3: 0.591667\n",
      "epoch   4 |   100 batches loss: 1.9997\n",
      "[Epoch   4]: Training loss: 1.961720 | Accuracy: 0.448611\n",
      "[Epoch   4]: Validation loss: 1.945796 | Accuracy: 0.434722 | Within 3: 0.673611\n",
      "epoch   5 |   100 batches loss: 1.7502\n",
      "[Epoch   5]: Training loss: 1.737302 | Accuracy: 0.518056\n",
      "[Epoch   5]: Validation loss: 1.823649 | Accuracy: 0.470833 | Within 3: 0.700000\n",
      "epoch   6 |   100 batches loss: 1.5859\n",
      "[Epoch   6]: Training loss: 1.562328 | Accuracy: 0.556349\n",
      "[Epoch   6]: Validation loss: 1.728986 | Accuracy: 0.488889 | Within 3: 0.718056\n",
      "epoch   7 |   100 batches loss: 1.4085\n",
      "[Epoch   7]: Training loss: 1.405028 | Accuracy: 0.596429\n",
      "[Epoch   7]: Validation loss: 1.659597 | Accuracy: 0.538889 | Within 3: 0.734722\n",
      "epoch   8 |   100 batches loss: 1.2911\n",
      "[Epoch   8]: Training loss: 1.278972 | Accuracy: 0.635714\n",
      "[Epoch   8]: Validation loss: 1.582711 | Accuracy: 0.538889 | Within 3: 0.751389\n",
      "epoch   9 |   100 batches loss: 1.1504\n",
      "[Epoch   9]: Training loss: 1.159165 | Accuracy: 0.667262\n",
      "[Epoch   9]: Validation loss: 1.528563 | Accuracy: 0.548611 | Within 3: 0.765278\n",
      "epoch  10 |   100 batches loss: 1.0410\n",
      "[Epoch  10]: Training loss: 1.052638 | Accuracy: 0.696230\n",
      "[Epoch  10]: Validation loss: 1.519380 | Accuracy: 0.552778 | Within 3: 0.756944\n",
      "epoch  11 |   100 batches loss: 0.9605\n",
      "[Epoch  11]: Training loss: 0.963465 | Accuracy: 0.724603\n",
      "[Epoch  11]: Validation loss: 1.491196 | Accuracy: 0.555556 | Within 3: 0.768056\n",
      "epoch  12 |   100 batches loss: 0.8680\n",
      "[Epoch  12]: Training loss: 0.877625 | Accuracy: 0.752579\n",
      "[Epoch  12]: Validation loss: 1.494058 | Accuracy: 0.569444 | Within 3: 0.781944\n",
      "epoch  13 |   100 batches loss: 0.7932\n",
      "[Epoch  13]: Training loss: 0.799044 | Accuracy: 0.777579\n",
      "[Epoch  13]: Validation loss: 1.473231 | Accuracy: 0.584722 | Within 3: 0.784722\n",
      "epoch  14 |   100 batches loss: 0.7143\n",
      "[Epoch  14]: Training loss: 0.728486 | Accuracy: 0.799802\n",
      "[Epoch  14]: Validation loss: 1.454157 | Accuracy: 0.588889 | Within 3: 0.788889\n",
      "epoch  15 |   100 batches loss: 0.6622\n",
      "[Epoch  15]: Training loss: 0.661164 | Accuracy: 0.817460\n",
      "[Epoch  15]: Validation loss: 1.447437 | Accuracy: 0.593056 | Within 3: 0.794444\n",
      "epoch  16 |   100 batches loss: 0.6056\n",
      "[Epoch  16]: Training loss: 0.604819 | Accuracy: 0.833929\n",
      "[Epoch  16]: Validation loss: 1.505734 | Accuracy: 0.584722 | Within 3: 0.781944\n",
      "epoch  17 |   100 batches loss: 0.5363\n",
      "[Epoch  17]: Training loss: 0.546172 | Accuracy: 0.857143\n",
      "[Epoch  17]: Validation loss: 1.453003 | Accuracy: 0.606944 | Within 3: 0.798611\n",
      "epoch  18 |   100 batches loss: 0.4970\n",
      "[Epoch  18]: Training loss: 0.498907 | Accuracy: 0.871230\n",
      "[Epoch  18]: Validation loss: 1.464422 | Accuracy: 0.606944 | Within 3: 0.795833\n",
      "epoch  19 |   100 batches loss: 0.4466\n",
      "[Epoch  19]: Training loss: 0.448517 | Accuracy: 0.889087\n",
      "[Epoch  19]: Validation loss: 1.463310 | Accuracy: 0.587500 | Within 3: 0.795833\n",
      "epoch  20 |   100 batches loss: 0.4094\n",
      "[Epoch  20]: Training loss: 0.406266 | Accuracy: 0.897024\n",
      "[Epoch  20]: Validation loss: 1.445477 | Accuracy: 0.598611 | Within 3: 0.815278\n",
      "epoch  21 |   100 batches loss: 0.3564\n",
      "[Epoch  21]: Training loss: 0.363902 | Accuracy: 0.910119\n",
      "[Epoch  21]: Validation loss: 1.459419 | Accuracy: 0.609722 | Within 3: 0.813889\n",
      "epoch  22 |   100 batches loss: 0.3123\n",
      "[Epoch  22]: Training loss: 0.321011 | Accuracy: 0.925397\n",
      "[Epoch  22]: Validation loss: 1.502202 | Accuracy: 0.618056 | Within 3: 0.815278\n",
      "epoch  23 |   100 batches loss: 0.2936\n",
      "[Epoch  23]: Training loss: 0.286460 | Accuracy: 0.937897\n",
      "[Epoch  23]: Validation loss: 1.485467 | Accuracy: 0.622222 | Within 3: 0.808333\n",
      "epoch  24 |   100 batches loss: 0.2565\n",
      "[Epoch  24]: Training loss: 0.255184 | Accuracy: 0.946825\n",
      "[Epoch  24]: Validation loss: 1.491267 | Accuracy: 0.611111 | Within 3: 0.820833\n",
      "epoch  25 |   100 batches loss: 0.2150\n",
      "[Epoch  25]: Training loss: 0.225325 | Accuracy: 0.954762\n",
      "[Epoch  25]: Validation loss: 1.547285 | Accuracy: 0.608333 | Within 3: 0.806944\n",
      "epoch  26 |   100 batches loss: 0.1863\n",
      "[Epoch  26]: Training loss: 0.192864 | Accuracy: 0.966270\n",
      "[Epoch  26]: Validation loss: 1.593142 | Accuracy: 0.606944 | Within 3: 0.802778\n",
      "epoch  27 |   100 batches loss: 0.1628\n",
      "[Epoch  27]: Training loss: 0.170549 | Accuracy: 0.972421\n",
      "[Epoch  27]: Validation loss: 1.570415 | Accuracy: 0.613889 | Within 3: 0.805556\n",
      "epoch  28 |   100 batches loss: 0.1466\n",
      "[Epoch  28]: Training loss: 0.146800 | Accuracy: 0.979167\n",
      "[Epoch  28]: Validation loss: 1.523576 | Accuracy: 0.627778 | Within 3: 0.809722\n",
      "epoch  29 |   100 batches loss: 0.1222\n",
      "[Epoch  29]: Training loss: 0.127154 | Accuracy: 0.982341\n",
      "[Epoch  29]: Validation loss: 1.627489 | Accuracy: 0.606944 | Within 3: 0.811111\n",
      "Best epoch:  28\n"
     ]
    }
   ],
   "source": [
    "mtrainer.run_train(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c903eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 60.27777777777777 %\n",
      "Accuracy within top 3 results: 79.44444444444444 %\n",
      "(Home-Appliances, Automotive): 15\n",
      "(Women's-Shoes, Men's-Shoes): 12\n",
      "(Hobbies-Books, Toys-Kids-Babies): 12\n",
      "(Beauty-Personal-Care, Home-Living): 12\n",
      "(Men's-Bags, Women's-Bags): 11\n",
      "(Home-Living, Toys-Kids-Babies): 10\n",
      "(Women's-Bags, Men's-Bags): 9\n",
      "(Kids-Fashion, Toys-Kids-Babies): 9\n",
      "(Computers-Peripherals, Cameras-Drones): 9\n",
      "(Computers-Peripherals, Mobile-Gadgets): 9\n",
      "(Health-Wellness, Toys-Kids-Babies): 7\n",
      "(Computers-Peripherals, Hobbies-Books): 7\n",
      "(Home-Appliances, Pet-Food-Supplies): 7\n",
      "(Mobile-Gadgets, Cameras-Drones): 7\n",
      "(Beauty-Personal-Care, Toys-Kids-Babies): 7\n",
      "(Men's-Shoes, Women's-Shoes): 7\n",
      "(Mobile-Gadgets, Watches): 6\n",
      "(Beauty-Personal-Care, Automotive): 6\n",
      "(Travel-Luggage, Men's-Bags): 6\n",
      "(Home-Appliances, Home-Living): 5\n",
      "(Health-Wellness, Home-Living): 5\n",
      "(Women's-Apparel, Jewellery-Accessories): 5\n",
      "(Hobbies-Books, Cameras-Drones): 5\n",
      "(Men's-Shoes, Sports-Outdoors): 5\n",
      "(Home-Living, Automotive): 5\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, top_k, incorect_stats = mtrainer.run_test(mtrainer.data.testloader, 3, True)\n",
    "print(f'Accuracy of the network on the test images: {test_acc*100} %')\n",
    "print(f'Accuracy within top 3 results: {top_k*100} %')\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter(incorect_stats).most_common(25)\n",
    "for k, v in counts:\n",
    "    print(f\"({data.categories[k[0]]}, {data.categories[k[1]]}): {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae836900",
   "metadata": {},
   "source": [
    "## CNN Model with skip connections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d88d51e5feeef7cdbc38f54879bc8b14b595db7785db4e3b195d2593607adbb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

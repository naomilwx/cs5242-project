{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cef52a0",
   "metadata": {},
   "source": [
    "# CS5242 Shopee Product Classification: Other Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aed808",
   "metadata": {},
   "source": [
    "* In this notebook, we aim to use some of the other types of Neural network building blocks to perform image classification.\n",
    "* These building blocks are added on to our CNN baseline model and evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc650fb",
   "metadata": {},
   "source": [
    "The two additional types of network experiments performed in this notebook are as follows:\n",
    "\n",
    "* Recurrent Neural Networks (RNN)\n",
    "* Attention Neural Networks (Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d703a",
   "metadata": {},
   "source": [
    "## Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b64a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model import dataset, trainer\n",
    "from model import baseline_cnn_1, rnn_cnn, attention_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38cd9d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epoch = 30\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d493f4",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca6b948",
   "metadata": {},
   "source": [
    "* As previously, we use our dataset to import the set of images across categories.\n",
    "* The 9 categories are selected with the custom filtered 500 images from each of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.DataSet(max_num_img=3000, crop=0.8, path='data/selected_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7adc10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.load_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76fb4af",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2143894",
   "metadata": {},
   "source": [
    "* Before we proceed with these networks, we add in one evaluation of our baseline model to enable us to compare performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2330d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_cnn_1_model = baseline_cnn_1.BaselineCNN1(len(data.categories))\n",
    "torch.manual_seed(seed)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(baseline_cnn_1_model.parameters(), lr=4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578b90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrainer = trainer.Trainer(baseline_cnn_1_model, optimizer, criterion, data, batch_size)\n",
    "mtrainer.run_train(num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, top_k, incorect_stats = mtrainer.run_test(mtrainer.testloader, 3, True)\n",
    "print(f'Accuracy of the network on the test images: {test_acc*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0659c922",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf786b0",
   "metadata": {},
   "source": [
    "* In this approach, we add an RNN layer over the baseline CNN model we implemented.\n",
    "* The RNN layer selected is a Long Short Term Memory (LSTM) layer from the Pytorch nn modules.\n",
    "    * We keep all other convolutional blocks the same as compared to the baseline CNN model.\n",
    "* The LSTM mechanism is implemented as follows:\n",
    "    * After passing through the convolutional blocks, the image is split into smaller patches\n",
    "    * These patches are then passed sequentially into the LSTM model.\n",
    "    * The number of hidden states in the LSTM is directly proportional to the number of patches in the image.\n",
    "* Following the LSTM layer, a final fully connected layer is used.\n",
    "    * The adaptive average pooling layer is removed in this case.\n",
    "\n",
    "The RNN and CNN model was experimented with, owing to findings from https://www.matec-conferences.org/articles/matecconf/pdf/2019/26/matecconf_jcmme2018_02001.pdf following a similar approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cnn_model = rnn_cnn.CNNWithRNN(len(data.categories))\n",
    "torch.manual_seed(seed)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(baseline_cnn_1_model.parameters(), lr=4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrainer = trainer.Trainer(baseline_cnn_1_model, optimizer, criterion, data, batch_size)\n",
    "mtrainer.run_train(num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e50889",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, top_k, incorect_stats = mtrainer.run_test(mtrainer.testloader, 3, True)\n",
    "print(f'Accuracy of the network on the test images: {test_acc*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65257700",
   "metadata": {},
   "source": [
    "* We can see that the RNN model did not do as well as our baseline model and in fact led to a small reduction in performance.\n",
    "* In order to further understand this, we performed some paramter tuning on our model to see if that would affect our results, the results of which are explained below.\n",
    "\n",
    "* **Increase in patch size**:\n",
    "    * The increase in patch size led to a reduced performance on the RNN. This made sense since a larger patch size would require more information to be incorporated by the hidden cells and would lead to higher loss.\n",
    "* **More stacked layers**:\n",
    "    * Stacking multiple LSTM layers helped to increase the depth of our model and learn more features. We noticed that stacking 2 layers helped to provide a small improvement in the score, but increasing it to 3 led to a reduction. Thus stacking too many layers led to a higher degree of overfitting.\n",
    "* **Removing MaxPool after convolution**:\n",
    "    * An experiment was run with removing the MaxPool after the convolution layers as well, with the expectation that this would reduce abstraction and provide more data to the RNN. However this seemed to make performance worse as well. It would appear that the maxpool is important before applying the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978cece1",
   "metadata": {},
   "source": [
    "## Attention Neural Network (Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c7933",
   "metadata": {},
   "source": [
    "* In this approach, attention blocks are added after the convolution layers of the baseline model.\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2042725a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

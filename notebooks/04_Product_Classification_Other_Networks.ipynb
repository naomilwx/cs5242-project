{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cef52a0",
   "metadata": {
    "id": "0cef52a0"
   },
   "source": [
    "# Shopee Product Classification: Other Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aed808",
   "metadata": {
    "id": "94aed808"
   },
   "source": [
    "* In this notebook, we aim to use some of the other types of Neural network building blocks to perform image classification.\n",
    "* These building blocks are added on to our CNN baseline model and evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc650fb",
   "metadata": {
    "id": "9fc650fb"
   },
   "source": [
    "The two additional types of network experiments performed in this notebook are as follows:\n",
    "\n",
    "* Recurrent Neural Networks (RNN)\n",
    "* Attention Neural Networks (Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d703a",
   "metadata": {
    "id": "6e6d703a"
   },
   "source": [
    "## Imports and Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cfaf24",
   "metadata": {},
   "source": [
    "Note: This notebook was run in colab with this notebook in the root of the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "GPyX0-xjdAaC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPyX0-xjdAaC",
    "outputId": "302c2823-bce6-497f-e456-783e8abd925e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# # Uncomment and run if running with file on drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "# import os\n",
    "# os.chdir('gdrive/MyDrive/cs5242-project/cs5242-project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b64a85",
   "metadata": {
    "id": "66b64a85"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model import dataset, trainer\n",
    "from model import baseline_cnn_1, rnn_cnn, attention_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38cd9d59",
   "metadata": {
    "id": "38cd9d59"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epoch = 30\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d493f4",
   "metadata": {
    "id": "a3d493f4"
   },
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca6b948",
   "metadata": {
    "id": "3ca6b948"
   },
   "source": [
    "* As previously, we use our dataset to import the set of images across categories.\n",
    "* The 9 categories are selected with the custom filtered 500 images from each of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6886afb2",
   "metadata": {
    "id": "6886afb2"
   },
   "outputs": [],
   "source": [
    "image_dir = 'data/selected_images/'\n",
    "\n",
    "#### Uncomment the following block if running from the `notebooks` folder\n",
    "# import sys\n",
    "# sys.path.insert(0, '../')\n",
    "# image_dir = '../data/selected_images/'\n",
    "#####\n",
    "\n",
    "data = dataset.DataSet(max_num_img=500, crop=0.8, path=image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7adc10b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7adc10b",
    "outputId": "71d5f9a0-ed4c-4210-ef7c-24bcece28cb2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:11<00:00, 43.08it/s] \n",
      "100%|██████████| 500/500 [06:52<00:00,  1.21it/s]\n",
      "100%|██████████| 500/500 [06:51<00:00,  1.21it/s]\n",
      "100%|██████████| 500/500 [06:51<00:00,  1.21it/s]\n",
      "100%|██████████| 500/500 [06:52<00:00,  1.21it/s]\n",
      "100%|██████████| 500/500 [06:56<00:00,  1.20it/s]\n",
      "100%|██████████| 500/500 [06:58<00:00,  1.20it/s]\n",
      "100%|██████████| 500/500 [06:49<00:00,  1.22it/s]\n",
      "100%|██████████| 500/500 [07:00<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "data.load_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76fb4af",
   "metadata": {
    "id": "d76fb4af"
   },
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2143894",
   "metadata": {
    "id": "f2143894"
   },
   "source": [
    "* Before we proceed with these networks, we add in one evaluation of our baseline model to enable us to compare performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2330d82",
   "metadata": {
    "id": "b2330d82"
   },
   "outputs": [],
   "source": [
    "baseline_cnn_1_model = baseline_cnn_1.BaselineCNN1(len(data.categories))\n",
    "torch.manual_seed(seed)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(baseline_cnn_1_model.parameters(), lr=4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "578b90fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "578b90fe",
    "outputId": "71208a82-14c5-4bec-f106-7f62547573a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   0]: Training loss: 1.735802 | Accuracy: 0.383810\n",
      "[Epoch   0]: Validation loss: 1.682558 | Accuracy: 0.377778 | Within 3: 0.740000\n",
      "[Epoch   1]: Training loss: 1.558757 | Accuracy: 0.465079\n",
      "[Epoch   1]: Validation loss: 1.587951 | Accuracy: 0.455556 | Within 3: 0.773333\n",
      "[Epoch   2]: Training loss: 1.455608 | Accuracy: 0.507937\n",
      "[Epoch   2]: Validation loss: 1.503304 | Accuracy: 0.484444 | Within 3: 0.766667\n",
      "[Epoch   3]: Training loss: 1.386036 | Accuracy: 0.522222\n",
      "[Epoch   3]: Validation loss: 1.781648 | Accuracy: 0.420000 | Within 3: 0.691111\n",
      "[Epoch   4]: Training loss: 1.323803 | Accuracy: 0.547619\n",
      "[Epoch   4]: Validation loss: 1.615260 | Accuracy: 0.440000 | Within 3: 0.733333\n",
      "[Epoch   5]: Training loss: 1.242632 | Accuracy: 0.577778\n",
      "[Epoch   5]: Validation loss: 1.393525 | Accuracy: 0.515556 | Within 3: 0.797778\n",
      "[Epoch   6]: Training loss: 1.197349 | Accuracy: 0.598413\n",
      "[Epoch   6]: Validation loss: 1.489513 | Accuracy: 0.526667 | Within 3: 0.802222\n",
      "[Epoch   7]: Training loss: 1.160527 | Accuracy: 0.610794\n",
      "[Epoch   7]: Validation loss: 1.328333 | Accuracy: 0.588889 | Within 3: 0.837778\n",
      "[Epoch   8]: Training loss: 1.112475 | Accuracy: 0.624444\n",
      "[Epoch   8]: Validation loss: 1.340035 | Accuracy: 0.560000 | Within 3: 0.813333\n",
      "[Epoch   9]: Training loss: 1.045686 | Accuracy: 0.645714\n",
      "[Epoch   9]: Validation loss: 1.283999 | Accuracy: 0.568889 | Within 3: 0.851111\n",
      "[Epoch  10]: Training loss: 1.010079 | Accuracy: 0.661905\n",
      "[Epoch  10]: Validation loss: 1.345264 | Accuracy: 0.537778 | Within 3: 0.824444\n",
      "[Epoch  11]: Training loss: 0.957164 | Accuracy: 0.678095\n",
      "[Epoch  11]: Validation loss: 1.346428 | Accuracy: 0.602222 | Within 3: 0.837778\n",
      "[Epoch  12]: Training loss: 0.919005 | Accuracy: 0.699365\n",
      "[Epoch  12]: Validation loss: 1.437072 | Accuracy: 0.553333 | Within 3: 0.844444\n",
      "[Epoch  13]: Training loss: 0.862388 | Accuracy: 0.717778\n",
      "[Epoch  13]: Validation loss: 1.314446 | Accuracy: 0.600000 | Within 3: 0.853333\n",
      "[Epoch  14]: Training loss: 0.797262 | Accuracy: 0.737460\n",
      "[Epoch  14]: Validation loss: 1.341163 | Accuracy: 0.553333 | Within 3: 0.831111\n",
      "[Epoch  15]: Training loss: 0.766387 | Accuracy: 0.756825\n",
      "[Epoch  15]: Validation loss: 1.594670 | Accuracy: 0.555556 | Within 3: 0.797778\n",
      "[Epoch  16]: Training loss: 0.692109 | Accuracy: 0.769206\n",
      "[Epoch  16]: Validation loss: 1.265807 | Accuracy: 0.617778 | Within 3: 0.877778\n",
      "[Epoch  17]: Training loss: 0.660796 | Accuracy: 0.793333\n",
      "[Epoch  17]: Validation loss: 1.621112 | Accuracy: 0.591111 | Within 3: 0.840000\n",
      "[Epoch  18]: Training loss: 0.593918 | Accuracy: 0.807302\n",
      "[Epoch  18]: Validation loss: 1.128772 | Accuracy: 0.657778 | Within 3: 0.888889\n",
      "[Epoch  19]: Training loss: 0.530623 | Accuracy: 0.838413\n",
      "[Epoch  19]: Validation loss: 1.195977 | Accuracy: 0.635556 | Within 3: 0.860000\n",
      "[Epoch  20]: Training loss: 0.489469 | Accuracy: 0.848571\n",
      "[Epoch  20]: Validation loss: 1.143129 | Accuracy: 0.642222 | Within 3: 0.884444\n",
      "[Epoch  21]: Training loss: 0.418537 | Accuracy: 0.881270\n",
      "[Epoch  21]: Validation loss: 1.168480 | Accuracy: 0.651111 | Within 3: 0.864444\n",
      "[Epoch  22]: Training loss: 0.366025 | Accuracy: 0.901270\n",
      "[Epoch  22]: Validation loss: 1.607844 | Accuracy: 0.544444 | Within 3: 0.831111\n",
      "[Epoch  23]: Training loss: 0.315517 | Accuracy: 0.921270\n",
      "[Epoch  23]: Validation loss: 1.308049 | Accuracy: 0.620000 | Within 3: 0.862222\n",
      "[Epoch  24]: Training loss: 0.247704 | Accuracy: 0.941905\n",
      "[Epoch  24]: Validation loss: 1.025510 | Accuracy: 0.671111 | Within 3: 0.904444\n",
      "[Epoch  25]: Training loss: 0.229621 | Accuracy: 0.947937\n",
      "[Epoch  25]: Validation loss: 1.023675 | Accuracy: 0.697778 | Within 3: 0.902222\n",
      "[Epoch  26]: Training loss: 0.209746 | Accuracy: 0.950476\n",
      "[Epoch  26]: Validation loss: 1.263489 | Accuracy: 0.640000 | Within 3: 0.888889\n",
      "[Epoch  27]: Training loss: 0.187066 | Accuracy: 0.954921\n",
      "[Epoch  27]: Validation loss: 1.219914 | Accuracy: 0.677778 | Within 3: 0.888889\n",
      "[Epoch  28]: Training loss: 0.144981 | Accuracy: 0.972063\n",
      "[Epoch  28]: Validation loss: 1.131426 | Accuracy: 0.675556 | Within 3: 0.884444\n",
      "[Epoch  29]: Training loss: 0.092850 | Accuracy: 0.989524\n",
      "[Epoch  29]: Validation loss: 1.145707 | Accuracy: 0.684444 | Within 3: 0.891111\n",
      "Best epoch:  25\n"
     ]
    }
   ],
   "source": [
    "mtrainer = trainer.Trainer(baseline_cnn_1_model, optimizer, criterion, data, batch_size)\n",
    "mtrainer.run_train(num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e9a62cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e9a62cf",
    "outputId": "f16e8e98-2378-4301-c31e-f65e24bee6dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 64.55555555555556 %\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, top_k, incorect_stats = mtrainer.run_test(mtrainer.testloader, 3, True)\n",
    "print(f'Accuracy of the network on the test images: {test_acc*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0659c922",
   "metadata": {
    "id": "0659c922"
   },
   "source": [
    "## Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf786b0",
   "metadata": {
    "id": "8bf786b0"
   },
   "source": [
    "* In this approach, we add an RNN layer over the baseline CNN model we implemented.\n",
    "* The RNN layer selected is a Long Short Term Memory (LSTM) layer from the Pytorch nn modules.\n",
    "    * We keep all other convolutional blocks the same as compared to the baseline CNN model.\n",
    "* The LSTM mechanism is implemented as follows:\n",
    "    * After passing through the convolutional blocks, the image is split into smaller patches\n",
    "    * These patches are then passed sequentially into the LSTM model.\n",
    "    * The number of hidden states in the LSTM is directly proportional to the number of patches in the image.\n",
    "* Following the LSTM layer, a final fully connected layer is used.\n",
    "    * The adaptive average pooling layer is removed in this case.\n",
    "\n",
    "The RNN and CNN model was experimented with, owing to findings from https://www.matec-conferences.org/articles/matecconf/pdf/2019/26/matecconf_jcmme2018_02001.pdf following a similar approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ce6b727",
   "metadata": {
    "id": "2ce6b727"
   },
   "outputs": [],
   "source": [
    "rnn_cnn_model = rnn_cnn.CNNWithRNN(len(data.categories))\n",
    "torch.manual_seed(seed)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn_cnn_model.parameters(), lr=4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd9b270",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dd9b270",
    "outputId": "ad108e24-94b9-4727-d9f5-e37edf55a7ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   0]: Training loss: 2.267701 | Accuracy: 0.180635\n",
      "[Epoch   0]: Validation loss: 2.187227 | Accuracy: 0.215556 | Within 3: 0.548889\n",
      "[Epoch   1]: Training loss: 1.861043 | Accuracy: 0.315873\n",
      "[Epoch   1]: Validation loss: 1.695775 | Accuracy: 0.368889 | Within 3: 0.700000\n",
      "[Epoch   2]: Training loss: 1.667541 | Accuracy: 0.401587\n",
      "[Epoch   2]: Validation loss: 1.628065 | Accuracy: 0.384444 | Within 3: 0.768889\n",
      "[Epoch   3]: Training loss: 1.530685 | Accuracy: 0.463492\n",
      "[Epoch   3]: Validation loss: 1.637616 | Accuracy: 0.384444 | Within 3: 0.726667\n",
      "[Epoch   4]: Training loss: 1.412013 | Accuracy: 0.506349\n",
      "[Epoch   4]: Validation loss: 1.464527 | Accuracy: 0.480000 | Within 3: 0.797778\n",
      "[Epoch   5]: Training loss: 1.299016 | Accuracy: 0.543492\n",
      "[Epoch   5]: Validation loss: 1.504222 | Accuracy: 0.442222 | Within 3: 0.764444\n",
      "[Epoch   6]: Training loss: 1.220570 | Accuracy: 0.569206\n",
      "[Epoch   6]: Validation loss: 1.291953 | Accuracy: 0.551111 | Within 3: 0.831111\n",
      "[Epoch   7]: Training loss: 1.151721 | Accuracy: 0.609206\n",
      "[Epoch   7]: Validation loss: 1.930485 | Accuracy: 0.402222 | Within 3: 0.715556\n",
      "[Epoch   8]: Training loss: 1.088983 | Accuracy: 0.627302\n",
      "[Epoch   8]: Validation loss: 1.345727 | Accuracy: 0.533333 | Within 3: 0.842222\n",
      "[Epoch   9]: Training loss: 1.033517 | Accuracy: 0.641270\n",
      "[Epoch   9]: Validation loss: 1.388131 | Accuracy: 0.566667 | Within 3: 0.837778\n",
      "[Epoch  10]: Training loss: 0.962159 | Accuracy: 0.663810\n",
      "[Epoch  10]: Validation loss: 1.473181 | Accuracy: 0.533333 | Within 3: 0.786667\n",
      "[Epoch  11]: Training loss: 0.894041 | Accuracy: 0.698730\n",
      "[Epoch  11]: Validation loss: 1.368866 | Accuracy: 0.582222 | Within 3: 0.848889\n",
      "[Epoch  12]: Training loss: 0.865153 | Accuracy: 0.710794\n",
      "[Epoch  12]: Validation loss: 1.508385 | Accuracy: 0.542222 | Within 3: 0.848889\n",
      "[Epoch  13]: Training loss: 0.777739 | Accuracy: 0.737143\n",
      "[Epoch  13]: Validation loss: 1.411547 | Accuracy: 0.617778 | Within 3: 0.844444\n",
      "[Epoch  14]: Training loss: 0.740714 | Accuracy: 0.745714\n",
      "[Epoch  14]: Validation loss: 1.448089 | Accuracy: 0.555556 | Within 3: 0.826667\n",
      "[Epoch  15]: Training loss: 0.677032 | Accuracy: 0.761905\n",
      "[Epoch  15]: Validation loss: 1.444462 | Accuracy: 0.588889 | Within 3: 0.842222\n",
      "[Epoch  16]: Training loss: 0.604514 | Accuracy: 0.795873\n",
      "[Epoch  16]: Validation loss: 1.269851 | Accuracy: 0.606667 | Within 3: 0.855556\n",
      "[Epoch  17]: Training loss: 0.556925 | Accuracy: 0.806032\n",
      "[Epoch  17]: Validation loss: 1.786191 | Accuracy: 0.506667 | Within 3: 0.804444\n",
      "[Epoch  18]: Training loss: 0.456945 | Accuracy: 0.843492\n",
      "[Epoch  18]: Validation loss: 1.433067 | Accuracy: 0.582222 | Within 3: 0.844444\n",
      "[Epoch  19]: Training loss: 0.408629 | Accuracy: 0.860000\n",
      "[Epoch  19]: Validation loss: 1.897093 | Accuracy: 0.571111 | Within 3: 0.828889\n",
      "[Epoch  20]: Training loss: 0.362043 | Accuracy: 0.884127\n",
      "[Epoch  20]: Validation loss: 1.570281 | Accuracy: 0.588889 | Within 3: 0.846667\n",
      "[Epoch  21]: Training loss: 0.296772 | Accuracy: 0.902222\n",
      "[Epoch  21]: Validation loss: 1.708306 | Accuracy: 0.613333 | Within 3: 0.840000\n",
      "[Epoch  22]: Training loss: 0.230484 | Accuracy: 0.928889\n",
      "[Epoch  22]: Validation loss: 1.566925 | Accuracy: 0.631111 | Within 3: 0.848889\n",
      "[Epoch  23]: Training loss: 0.205052 | Accuracy: 0.935873\n",
      "[Epoch  23]: Validation loss: 1.492299 | Accuracy: 0.655556 | Within 3: 0.873333\n",
      "[Epoch  24]: Training loss: 0.146249 | Accuracy: 0.954286\n",
      "[Epoch  24]: Validation loss: 1.537995 | Accuracy: 0.620000 | Within 3: 0.868889\n",
      "[Epoch  25]: Training loss: 0.138356 | Accuracy: 0.960317\n",
      "[Epoch  25]: Validation loss: 1.806990 | Accuracy: 0.635556 | Within 3: 0.866667\n",
      "[Epoch  26]: Training loss: 0.112917 | Accuracy: 0.970476\n",
      "[Epoch  26]: Validation loss: 3.018891 | Accuracy: 0.488889 | Within 3: 0.757778\n",
      "[Epoch  27]: Training loss: 0.089073 | Accuracy: 0.974921\n",
      "[Epoch  27]: Validation loss: 1.983309 | Accuracy: 0.608889 | Within 3: 0.855556\n",
      "[Epoch  28]: Training loss: 0.100229 | Accuracy: 0.974286\n",
      "[Epoch  28]: Validation loss: 2.099328 | Accuracy: 0.600000 | Within 3: 0.842222\n",
      "[Epoch  29]: Training loss: 0.066224 | Accuracy: 0.981587\n",
      "[Epoch  29]: Validation loss: 1.800480 | Accuracy: 0.644444 | Within 3: 0.877778\n",
      "Best epoch:  23\n"
     ]
    }
   ],
   "source": [
    "mtrainer = trainer.Trainer(rnn_cnn_model, optimizer, criterion, data, batch_size)\n",
    "mtrainer.run_train(num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89e50889",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89e50889",
    "outputId": "6b976d96-fadf-4c7a-8bbd-8832720df4d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 63.55555555555556 %\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, top_k, incorect_stats = mtrainer.run_test(mtrainer.testloader, 3, True)\n",
    "print(f'Accuracy of the network on the test images: {test_acc*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65257700",
   "metadata": {
    "id": "65257700"
   },
   "source": [
    "* We can see that the RNN model did not do as well as our baseline model and in fact led to a small reduction in performance (63.5% < 64.5%).\n",
    "* In order to further understand this, we performed some paramter tuning on our model to see if that would affect our results, the results of which are explained below.\n",
    "\n",
    "* **Increase in patch size**:\n",
    "    * The increase in patch size led to a reduced performance on the RNN. This made sense since a larger patch size would require more information to be incorporated by the hidden cells and would lead to higher loss.\n",
    "* **More stacked layers**:\n",
    "    * Stacking multiple LSTM layers helped to increase the depth of our model and learn more features. We noticed that stacking 2 layers helped to provide a small improvement in the score, but increasing it to 3 led to a reduction. Thus stacking too many layers led to a higher degree of overfitting.\n",
    "* **Removing MaxPool after convolution**:\n",
    "    * An experiment was run with removing the MaxPool after the convolution layers as well, with the expectation that this would reduce abstraction and provide more data to the RNN. However this seemed to make performance worse as well. It would appear that the maxpool is important before applying the RNN.\n",
    "\n",
    "* The result obtained above is after identifying the best parameters from search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978cece1",
   "metadata": {
    "id": "978cece1"
   },
   "source": [
    "## Attention Neural Network (Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c7933",
   "metadata": {
    "id": "f59c7933"
   },
   "source": [
    "* In the first approach, attention is applied at 2 points of the baseline model at increasing depth corresponding to increasing granuality of features extracted. The idea is to be able to weigh the different granularities of features extracted in the final classification decision, instead of just using the high level (global) features extracted by the last convolution layer.\n",
    "* Each attention vector is computed based on the local feature map at that point and the global feature map from the final convolutional layer. A custom attention layer is built which incorporates the following steps:\n",
    "    * The local feature map and the final global feature map are passed through a 1x1 convolutional block to project the features to a lower dimension.\n",
    "    * The global feature map is upsampled via bilinear interpolation to match the dimension of the local feature map.\n",
    "    * The feature map are then summed and projected to a single channel via another 1x1 convolution\n",
    "    * A softmax is then applied to the result to get the attention map.\n",
    "* The attention weights applied to each intermediate feature map are then concatennated with the output from the last feature layer and passed on to the classifier.\n",
    "\n",
    "This implementation is based on the approach proposed in the paper, [Melanoma Recognition via Visual Attention (Yan et al, 2019)](https://www2.cs.sfu.ca/~hamarneh/ecopy/ipmi2019.pdf) where the attention module was found to be helpful in improving the network's classification ability \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2042725a",
   "metadata": {
    "id": "2042725a"
   },
   "outputs": [],
   "source": [
    "attention_cnn_model = attention_cnn.CNNWithAttention(len(data.categories))\n",
    "torch.manual_seed(seed)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(attention_cnn_model.parameters(), lr=4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d562685",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d562685",
    "outputId": "be8aa573-45b9-46f0-cdae-6247b2162c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   0]: Training loss: 1.768142 | Accuracy: 0.369841\n",
      "[Epoch   0]: Validation loss: 1.673710 | Accuracy: 0.411111 | Within 3: 0.746667\n",
      "[Epoch   1]: Training loss: 1.598776 | Accuracy: 0.454603\n",
      "[Epoch   1]: Validation loss: 1.725220 | Accuracy: 0.415556 | Within 3: 0.724444\n",
      "[Epoch   2]: Training loss: 1.478499 | Accuracy: 0.501270\n",
      "[Epoch   2]: Validation loss: 1.664003 | Accuracy: 0.426667 | Within 3: 0.715556\n",
      "[Epoch   3]: Training loss: 1.395520 | Accuracy: 0.533016\n",
      "[Epoch   3]: Validation loss: 1.721746 | Accuracy: 0.415556 | Within 3: 0.700000\n",
      "[Epoch   4]: Training loss: 1.326853 | Accuracy: 0.539048\n",
      "[Epoch   4]: Validation loss: 1.404146 | Accuracy: 0.531111 | Within 3: 0.824444\n",
      "[Epoch   5]: Training loss: 1.244080 | Accuracy: 0.576508\n",
      "[Epoch   5]: Validation loss: 1.361586 | Accuracy: 0.500000 | Within 3: 0.815556\n",
      "[Epoch   6]: Training loss: 1.194026 | Accuracy: 0.593333\n",
      "[Epoch   6]: Validation loss: 1.369494 | Accuracy: 0.537778 | Within 3: 0.826667\n",
      "[Epoch   7]: Training loss: 1.145831 | Accuracy: 0.617143\n",
      "[Epoch   7]: Validation loss: 1.385464 | Accuracy: 0.540000 | Within 3: 0.820000\n",
      "[Epoch   8]: Training loss: 1.087339 | Accuracy: 0.632698\n",
      "[Epoch   8]: Validation loss: 1.341570 | Accuracy: 0.555556 | Within 3: 0.828889\n",
      "[Epoch   9]: Training loss: 1.014480 | Accuracy: 0.657143\n",
      "[Epoch   9]: Validation loss: 1.314315 | Accuracy: 0.580000 | Within 3: 0.860000\n",
      "[Epoch  10]: Training loss: 0.996827 | Accuracy: 0.660952\n",
      "[Epoch  10]: Validation loss: 1.243207 | Accuracy: 0.580000 | Within 3: 0.848889\n",
      "[Epoch  11]: Training loss: 0.934538 | Accuracy: 0.685079\n",
      "[Epoch  11]: Validation loss: 1.355860 | Accuracy: 0.555556 | Within 3: 0.842222\n",
      "[Epoch  12]: Training loss: 0.888702 | Accuracy: 0.706032\n",
      "[Epoch  12]: Validation loss: 1.455093 | Accuracy: 0.531111 | Within 3: 0.842222\n",
      "[Epoch  13]: Training loss: 0.865945 | Accuracy: 0.705079\n",
      "[Epoch  13]: Validation loss: 1.390618 | Accuracy: 0.582222 | Within 3: 0.848889\n",
      "[Epoch  14]: Training loss: 0.798145 | Accuracy: 0.731746\n",
      "[Epoch  14]: Validation loss: 1.141622 | Accuracy: 0.591111 | Within 3: 0.866667\n",
      "[Epoch  15]: Training loss: 0.717898 | Accuracy: 0.755238\n",
      "[Epoch  15]: Validation loss: 1.211970 | Accuracy: 0.617778 | Within 3: 0.875556\n",
      "[Epoch  16]: Training loss: 0.671330 | Accuracy: 0.776825\n",
      "[Epoch  16]: Validation loss: 1.110932 | Accuracy: 0.648889 | Within 3: 0.864444\n",
      "[Epoch  17]: Training loss: 0.649234 | Accuracy: 0.796508\n",
      "[Epoch  17]: Validation loss: 1.353965 | Accuracy: 0.571111 | Within 3: 0.837778\n",
      "[Epoch  18]: Training loss: 0.587495 | Accuracy: 0.814286\n",
      "[Epoch  18]: Validation loss: 1.238164 | Accuracy: 0.608889 | Within 3: 0.848889\n",
      "[Epoch  19]: Training loss: 0.531055 | Accuracy: 0.827302\n",
      "[Epoch  19]: Validation loss: 1.280557 | Accuracy: 0.582222 | Within 3: 0.853333\n",
      "[Epoch  20]: Training loss: 0.491722 | Accuracy: 0.835238\n",
      "[Epoch  20]: Validation loss: 1.377971 | Accuracy: 0.562222 | Within 3: 0.862222\n",
      "[Epoch  21]: Training loss: 0.484692 | Accuracy: 0.843175\n",
      "[Epoch  21]: Validation loss: 1.476613 | Accuracy: 0.573333 | Within 3: 0.860000\n",
      "[Epoch  22]: Training loss: 0.391982 | Accuracy: 0.878413\n",
      "[Epoch  22]: Validation loss: 1.242557 | Accuracy: 0.628889 | Within 3: 0.864444\n",
      "[Epoch  23]: Training loss: 0.365504 | Accuracy: 0.889841\n",
      "[Epoch  23]: Validation loss: 1.402276 | Accuracy: 0.557778 | Within 3: 0.837778\n",
      "[Epoch  24]: Training loss: 0.300985 | Accuracy: 0.911111\n",
      "[Epoch  24]: Validation loss: 1.258361 | Accuracy: 0.624444 | Within 3: 0.857778\n",
      "[Epoch  25]: Training loss: 0.332118 | Accuracy: 0.900000\n",
      "[Epoch  25]: Validation loss: 1.344076 | Accuracy: 0.617778 | Within 3: 0.866667\n",
      "[Epoch  26]: Training loss: 0.241973 | Accuracy: 0.932381\n",
      "[Epoch  26]: Validation loss: 1.266398 | Accuracy: 0.631111 | Within 3: 0.893333\n",
      "[Epoch  27]: Training loss: 0.220647 | Accuracy: 0.940317\n",
      "[Epoch  27]: Validation loss: 1.326414 | Accuracy: 0.648889 | Within 3: 0.860000\n",
      "[Epoch  28]: Training loss: 0.230885 | Accuracy: 0.933016\n",
      "[Epoch  28]: Validation loss: 1.439272 | Accuracy: 0.573333 | Within 3: 0.846667\n",
      "[Epoch  29]: Training loss: 0.177394 | Accuracy: 0.956508\n",
      "[Epoch  29]: Validation loss: 1.233052 | Accuracy: 0.666667 | Within 3: 0.871111\n",
      "Best epoch:  29\n"
     ]
    }
   ],
   "source": [
    "mtrainer = trainer.Trainer(attention_cnn_model, optimizer, criterion, data, batch_size)\n",
    "mtrainer.run_train(num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ee7ad66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ee7ad66",
    "outputId": "37a57d59-5e7a-4944-8ab7-6136d2e28933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 64.44444444444444 %\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, top_k, incorect_stats = mtrainer.run_test(mtrainer.testloader, 3, True)\n",
    "print(f'Accuracy of the network on the test images: {test_acc*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb6639",
   "metadata": {
    "id": "05cb6639"
   },
   "source": [
    "* We can see that the Attention network almost performs as good as the baseline on the test set (64.4% ~ 64.5%).\n",
    "* With the potential for improved performance, we tried another approach using an attention model which was based on a convolutional approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb980cf",
   "metadata": {},
   "source": [
    "### Convolutional Self-attention model\n",
    "* The convolution-like self-attention attention model uses a self attention block where attention is applied locally by iterating over the pixel regions in the image. This is based on the self-attention component from the paper, [Stand-Alone Self-Attention in Vision Models (Ramachandran et al, 2019)](https://arxiv.org/pdf/1906.05909.pdf). The paper claims that such self-attention blocks can replace convolutional blocks in a CNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "859ExaHNzfYx",
   "metadata": {
    "id": "859ExaHNzfYx"
   },
   "outputs": [],
   "source": [
    "attention_res_conv_cnn_model = attention_cnn.CNNWithConvAttention(len(data.categories))\n",
    "torch.manual_seed(seed)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(attention_res_conv_cnn_model.parameters(), lr=4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "X4mTGO_P0Pce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4mTGO_P0Pce",
    "outputId": "059f23b6-5b0b-4e0b-cebf-97d6e2b56e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   0]: Training loss: 1.733191 | Accuracy: 0.388571\n",
      "[Epoch   0]: Validation loss: 1.620318 | Accuracy: 0.440000 | Within 3: 0.744444\n",
      "[Epoch   1]: Training loss: 1.541552 | Accuracy: 0.458413\n",
      "[Epoch   1]: Validation loss: 1.573415 | Accuracy: 0.440000 | Within 3: 0.768889\n",
      "[Epoch   2]: Training loss: 1.411207 | Accuracy: 0.523810\n",
      "[Epoch   2]: Validation loss: 1.560676 | Accuracy: 0.464444 | Within 3: 0.728889\n",
      "[Epoch   3]: Training loss: 1.336674 | Accuracy: 0.539365\n",
      "[Epoch   3]: Validation loss: 1.459053 | Accuracy: 0.482222 | Within 3: 0.800000\n",
      "[Epoch   4]: Training loss: 1.249114 | Accuracy: 0.572698\n",
      "[Epoch   4]: Validation loss: 1.942010 | Accuracy: 0.380000 | Within 3: 0.740000\n",
      "[Epoch   5]: Training loss: 1.162595 | Accuracy: 0.607619\n",
      "[Epoch   5]: Validation loss: 1.456341 | Accuracy: 0.484444 | Within 3: 0.802222\n",
      "[Epoch   6]: Training loss: 1.106963 | Accuracy: 0.633016\n",
      "[Epoch   6]: Validation loss: 1.419516 | Accuracy: 0.544444 | Within 3: 0.793333\n",
      "[Epoch   7]: Training loss: 1.037960 | Accuracy: 0.658730\n",
      "[Epoch   7]: Validation loss: 1.279327 | Accuracy: 0.580000 | Within 3: 0.846667\n",
      "[Epoch   8]: Training loss: 0.966012 | Accuracy: 0.687937\n",
      "[Epoch   8]: Validation loss: 1.280972 | Accuracy: 0.584444 | Within 3: 0.868889\n",
      "[Epoch   9]: Training loss: 0.894733 | Accuracy: 0.697143\n",
      "[Epoch   9]: Validation loss: 1.317695 | Accuracy: 0.606667 | Within 3: 0.846667\n",
      "[Epoch  10]: Training loss: 0.826775 | Accuracy: 0.734286\n",
      "[Epoch  10]: Validation loss: 1.328210 | Accuracy: 0.604444 | Within 3: 0.857778\n",
      "[Epoch  11]: Training loss: 0.764348 | Accuracy: 0.755238\n",
      "[Epoch  11]: Validation loss: 1.337776 | Accuracy: 0.608889 | Within 3: 0.848889\n",
      "[Epoch  12]: Training loss: 0.705107 | Accuracy: 0.778095\n",
      "[Epoch  12]: Validation loss: 1.512133 | Accuracy: 0.555556 | Within 3: 0.835556\n",
      "[Epoch  13]: Training loss: 0.634395 | Accuracy: 0.802222\n",
      "[Epoch  13]: Validation loss: 1.448844 | Accuracy: 0.571111 | Within 3: 0.844444\n",
      "[Epoch  14]: Training loss: 0.555777 | Accuracy: 0.827937\n",
      "[Epoch  14]: Validation loss: 1.215201 | Accuracy: 0.626667 | Within 3: 0.884444\n",
      "[Epoch  15]: Training loss: 0.480233 | Accuracy: 0.853651\n",
      "[Epoch  15]: Validation loss: 1.468280 | Accuracy: 0.566667 | Within 3: 0.835556\n",
      "[Epoch  16]: Training loss: 0.424843 | Accuracy: 0.880000\n",
      "[Epoch  16]: Validation loss: 1.332965 | Accuracy: 0.580000 | Within 3: 0.837778\n",
      "[Epoch  17]: Training loss: 0.356419 | Accuracy: 0.901905\n",
      "[Epoch  17]: Validation loss: 1.388772 | Accuracy: 0.608889 | Within 3: 0.871111\n",
      "[Epoch  18]: Training loss: 0.292436 | Accuracy: 0.922857\n",
      "[Epoch  18]: Validation loss: 1.232802 | Accuracy: 0.615556 | Within 3: 0.853333\n",
      "[Epoch  19]: Training loss: 0.219540 | Accuracy: 0.954286\n",
      "[Epoch  19]: Validation loss: 1.769987 | Accuracy: 0.615556 | Within 3: 0.860000\n",
      "[Epoch  20]: Training loss: 0.178822 | Accuracy: 0.968254\n",
      "[Epoch  20]: Validation loss: 1.147258 | Accuracy: 0.646667 | Within 3: 0.902222\n",
      "[Epoch  21]: Training loss: 0.143575 | Accuracy: 0.979048\n",
      "[Epoch  21]: Validation loss: 1.398456 | Accuracy: 0.591111 | Within 3: 0.873333\n",
      "[Epoch  22]: Training loss: 0.115992 | Accuracy: 0.983175\n",
      "[Epoch  22]: Validation loss: 1.100205 | Accuracy: 0.662222 | Within 3: 0.893333\n",
      "[Epoch  23]: Training loss: 0.087266 | Accuracy: 0.987937\n",
      "[Epoch  23]: Validation loss: 1.066552 | Accuracy: 0.675556 | Within 3: 0.900000\n",
      "[Epoch  24]: Training loss: 0.074771 | Accuracy: 0.991111\n",
      "[Epoch  24]: Validation loss: 1.206444 | Accuracy: 0.657778 | Within 3: 0.873333\n",
      "[Epoch  25]: Training loss: 0.076818 | Accuracy: 0.988889\n",
      "[Epoch  25]: Validation loss: 1.139863 | Accuracy: 0.686667 | Within 3: 0.893333\n",
      "[Epoch  26]: Training loss: 0.084873 | Accuracy: 0.986667\n",
      "[Epoch  26]: Validation loss: 1.348579 | Accuracy: 0.651111 | Within 3: 0.882222\n",
      "[Epoch  27]: Training loss: 0.123427 | Accuracy: 0.972381\n",
      "[Epoch  27]: Validation loss: 1.682629 | Accuracy: 0.613333 | Within 3: 0.862222\n",
      "[Epoch  28]: Training loss: 0.092573 | Accuracy: 0.983175\n",
      "[Epoch  28]: Validation loss: 1.419036 | Accuracy: 0.620000 | Within 3: 0.864444\n",
      "[Epoch  29]: Training loss: 0.051063 | Accuracy: 0.994603\n",
      "[Epoch  29]: Validation loss: 1.041536 | Accuracy: 0.700000 | Within 3: 0.886667\n",
      "Best epoch:  29\n"
     ]
    }
   ],
   "source": [
    "mtrainer = trainer.Trainer(attention_res_conv_cnn_model, optimizer, criterion, data, batch_size)\n",
    "mtrainer.run_train(num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ePxmoP_T0SCR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePxmoP_T0SCR",
    "outputId": "713159e5-bbb1-4c48-e43e-08b4e34f4e3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 67.0 %\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, top_k, incorect_stats = mtrainer.run_test(mtrainer.testloader, 3, True)\n",
    "print(f'Accuracy of the network on the test images: {test_acc*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P-EMEwqs2dKw",
   "metadata": {
    "id": "P-EMEwqs2dKw"
   },
   "source": [
    "* The ConvAttention module seems to have given the best performance with a significant improvement over the baseline (67% > 64.5%).\n",
    "* We can thus see that local self-attention can be helpful in image classification."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
